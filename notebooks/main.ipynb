{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3781f022",
      "metadata": {
        "id": "3781f022"
      },
      "source": [
        "<h1>ü§ñ MLAI Workshop #03</h1>\n",
        "\n",
        "Within this workshop we will discuss the importance of problem formulation in machine learning and explore the limitations associated with different approaches. We will focus on evaluating our models.\n",
        "\n",
        "<h3>Agenda</h3>\n",
        "\n",
        "1.   **Trade-offs associated with the learning problem** - we will quickly recap some of the concepts from prior workshops.\n",
        "2.   **Explore problem formulation** - we will introduce problem formulation within the context of a simple classification problem.\n",
        "3.   **Explore the limitations** - we will then uncover and discuss the limitations of the approach and relate this back to our problem formulation.\n",
        "\n",
        "\n",
        "<h3>Outcomes</h3>\n",
        "\n",
        "1.   Gain an appreciation that every choice in the machine learning system imposes a biases about the solutions you can learn.\n",
        "2.   Gain an appreciation for finding the weaknesses in machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Section 0. The Learning Problem</h2>\n",
        "\n",
        "In the first workshop we introduced the trade-offs involved in the learning problem - specifically how each of the hypothesis, dataset, and optimization spaces influence the final solution $\\hat{f}$ we obtain.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://github.com/nextgenerationgraduatesprogram/nextgen25-mlai-workshop03/raw/main/media/notebook/LearningProblem.png\" height=\"600\"/>\n",
        "    <p><em>Figure 1. Illustration of the hypothesis, dataset, optimization, and target spaces considered in the learning problem.</em></p>\n",
        "</div>\n",
        "\n",
        "In the second workshop we built up a framework for performing flexible function approximation - which we used to approximate arbitrary functions over a defined domain.\n",
        "\n",
        "1. Define a dataset.\n",
        "2. Define a hypothesis space (model).\n",
        "3. Optimize the hypothesis space under the dataset.\n",
        "4. Analyse the results.\n",
        "\n",
        "We explored how different aspects of the learning problem influenced our performance on a toy problem. Specifically, we explored a of the form:\n",
        "\n",
        "\\begin{align*}\n",
        "  f: \\mathcal{X} \\to \\mathcal{Y} \\tag{1.1}\\\\\n",
        "\\end{align*}\n",
        "\n",
        "where $\\mathcal{X} \\in \\mathbb{R}^{1}$ and $\\mathcal{Y} \\in \\mathbb{R}^{1}$. This reflects a simple one-dimensional regression task which is useful for building intuition about aspects of the learning problem."
      ],
      "metadata": {
        "id": "_n9qX27xn1LK"
      },
      "id": "_n9qX27xn1LK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fif9AB-_g6u",
      "metadata": {
        "id": "1fif9AB-_g6u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from typing import Callable\n",
        "\n",
        "# create a dataset\n",
        "class Dataset:\n",
        "  def __init__(self, f: Callable, a: float, b: float, N: int = 100):\n",
        "    super(Dataset, self).__init__()\n",
        "\n",
        "    # Sample x uniformly in [a, b], shape [N,1]\n",
        "    self.x = torch.rand(N, 1) * (b - a) + a\n",
        "\n",
        "    # Observe f(x) through noise process, also [N,1]\n",
        "    self.y = f(self.x) + torch.randn(N, 1) * 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sM624eBF_g9J",
      "metadata": {
        "id": "sM624eBF_g9J"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# define a target function i.e. your data generating process\n",
        "f = lambda x: torch.exp(-x**2) * torch.sin(5.8 * torch.pi * x + 0.41)\n",
        "\n",
        "# define a dataset with your function\n",
        "dataset = Dataset(f, a=0, b=1, N=200)\n",
        "\n",
        "# visualize the observations\n",
        "fig, ax = plt.subplots(figsize=(8,2))\n",
        "ax.scatter(dataset.x, dataset.y, s=6, label=\"observations\")\n",
        "ax.set_xlim(dataset.x.min(), dataset.x.max())\n",
        "ax.legend(loc=\"best\")\n",
        "ax.set_xlabel(\"x_meas\")\n",
        "ax.set_ylabel(\"y_obs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DNCcOb1d_oGg",
      "metadata": {
        "id": "DNCcOb1d_oGg"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class PerceptronLayer(nn.Module):\n",
        "  \"\"\" single layer perceptron with a ReLU activation function as a non-linear basis\n",
        "  \"\"\"\n",
        "  def __init__(self, input_dim: int, output_dim: int, act: bool = True):\n",
        "    super(PerceptronLayer, self).__init__()\n",
        "    self.fc = nn.Linear(input_dim, output_dim)\n",
        "    self.act = nn.ReLU() if act else nn.Identity()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.act(self.fc(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "class MultiLayerPerceptron(nn.Module):\n",
        "  \"\"\" stack multiple layers of perceptrons to build more complex functions (mappings)\n",
        "  \"\"\"\n",
        "  def __init__(self, hidden_layers: int = 0, hidden_dim: int = 1, input_dim: int = 1, output_dim: int = 1):\n",
        "    super(MultiLayerPerceptron, self).__init__()\n",
        "\n",
        "    # define the input layer\n",
        "    modules = [PerceptronLayer(input_dim, hidden_dim)] # maps input_dim -> hidden_dim\n",
        "\n",
        "    # define the hidden layers\n",
        "    for _ in range(hidden_layers):\n",
        "      modules.append(PerceptronLayer(hidden_dim, hidden_dim)) # maps hidden_dim -> hidden_dim\n",
        "\n",
        "    # define the output layer\n",
        "    modules.append(PerceptronLayer(hidden_dim, output_dim, act=False)) # maps hidden_dim -> output_dim\n",
        "\n",
        "    # store the layers\n",
        "    self.layers = nn.ModuleList(modules) # store layers\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pvJ_FWL-_oJI",
      "metadata": {
        "id": "pvJ_FWL-_oJI"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# define a training loop\n",
        "def training_loop(model, optimizer, loss_fn, dataset, steps):\n",
        "  losses = []\n",
        "  with tqdm(range(steps)) as pbar:\n",
        "    for idx in pbar:\n",
        "      optimizer.zero_grad()\n",
        "      y_pred = model(dataset.x)\n",
        "      loss = loss_fn(dataset.y, y_pred)\n",
        "      losses.append(loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      pbar.set_description(f\"loss: {loss.item():.3f}\")\n",
        "    return model, torch.tensor(losses)\n",
        "\n",
        "# define some functions for plotting our results\n",
        "def plot_loss_curve(losses):\n",
        "    fig, ax = plt.subplots(figsize=(8,3))\n",
        "    ax.plot(losses, label=\"loss\")\n",
        "    ax.set_xlim(left=0, right=len(losses)-1)\n",
        "    ax.set_xlabel(\"Iteration\")\n",
        "    ax.set_ylabel(\"Loss (MSE)\")\n",
        "    ax.set_title(\"Loss Curve\")\n",
        "    ax.grid(True, alpha=0.50)\n",
        "    ax.legend()\n",
        "    return fig, ax\n",
        "\n",
        "def plot_predictions(model, dataset, ax = None):\n",
        "    if ax is None: fig, ax = plt.subplots(figsize=(8,3))\n",
        "    with torch.no_grad():\n",
        "        ax.scatter(dataset.x, model(dataset.x), s=4, label=\"predictions\")\n",
        "        ax.scatter(dataset.x, dataset.y, s=4, label=\"observations\")\n",
        "    ax.set_xlim(left=dataset.x.min(), right=dataset.x.max())\n",
        "    ax.grid(True, alpha=0.50)\n",
        "    ax.legend(loc=\"best\")\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WSmKl1KWAKe5",
      "metadata": {
        "id": "WSmKl1KWAKe5"
      },
      "source": [
        "When training our models we generally want our setup to have several properties:\n",
        "\n",
        "\n",
        "1.   We want the dataset to sufficiently represent the data generating process.\n",
        "2.   We want the hypothesis space to be large enough to accurately approximate the dataset.\n",
        "3.   We need our optimizer to be capable of finding a suitable set of solutions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ujoidm8j_xII",
      "metadata": {
        "id": "Ujoidm8j_xII"
      },
      "outputs": [],
      "source": [
        "# 1. Define a dataset\n",
        "dataset = Dataset(f, a=0, b=1, N=200)\n",
        "\n",
        "# 2. Define a hypothesis space\n",
        "model = MultiLayerPerceptron(hidden_layers=0, hidden_dim=10)\n",
        "\n",
        "# 3. Optimize the model under the dataset\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "model, losses = training_loop(model, optimizer, loss_fn, dataset, steps=500)\n",
        "\n",
        "# 4. Analyse the results\n",
        "plot_loss_curve(losses)\n",
        "plot_predictions(model, dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wnp9zEcJ_fhv",
      "metadata": {
        "id": "wnp9zEcJ_fhv"
      },
      "source": [
        "We also investigated some of the challenges inherent in learning from data in terms of interpolation and extrapolation performance. We saw that our interpolation error is typically bounded by our ability to resolve changes in the data manifold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C49lq_SzA_wg",
      "metadata": {
        "id": "C49lq_SzA_wg"
      },
      "outputs": [],
      "source": [
        "# 1. Define a dataset\n",
        "dataset = Dataset(f, a=0, b=1, N=500)\n",
        "\n",
        "# 1a. Erasing a region of the input domain\n",
        "x_min, x_max = 0.45, 0.60\n",
        "keep_idxs = (dataset.x < x_min) | (dataset.x > x_max)\n",
        "dataset.x = dataset.x[keep_idxs].unsqueeze(-1)\n",
        "dataset.y = dataset.y[keep_idxs].unsqueeze(-1)\n",
        "\n",
        "# 2. Define a hypothesis space\n",
        "model = MultiLayerPerceptron(hidden_layers=2, hidden_dim=20)\n",
        "\n",
        "# 3. Optimize the model under the dataset\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "model, losses = training_loop(model, optimizer, loss_fn, dataset, steps=500)\n",
        "\n",
        "# 4. Analyse the results\n",
        "plot_loss_curve(losses)\n",
        "ax = plot_predictions(model, dataset)\n",
        "\n",
        "# 4a. How well does it interpolate?\n",
        "dataset_interp = Dataset(f, a=x_min, b=x_max, N=100)\n",
        "ax = plot_predictions(model, dataset_interp, ax)\n",
        "ax.set_xlim(0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QUvK16t2A_5k",
      "metadata": {
        "id": "QUvK16t2A_5k"
      },
      "source": [
        "We extended this to the extrapolation region - where we perform inference on data outside the convex hull of our training distribution - without data to constrain the behaviour we typically observe poor generalization.\n",
        "\n",
        "Perfect generalization would require our network to have learned the function governing the data generating process - this isn't what our framework is designed for - our framework is design to approximate functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w3zPEdpeBAD7",
      "metadata": {
        "id": "w3zPEdpeBAD7"
      },
      "outputs": [],
      "source": [
        "# 4b. How well does it extrapolate?\n",
        "dataset_extrap = Dataset(f, -0.5, 1.5, 500)\n",
        "ax = plot_predictions(model, dataset_extrap)\n",
        "ax.set_xlim(-0.5, 1.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0reYaEsuBQVb",
      "metadata": {
        "id": "0reYaEsuBQVb"
      },
      "source": [
        "We expect this behaviour based on how our framework is defined - the learning problem is posed such that we learn to approximate the function defined by the training distribution. We built up our multi-layer perceptron to hierarchically construct a piecewise linear approximation of the function.\n",
        "\n",
        "Within this context, the representational capacity of neural networks is typically focussed on resolving the curvature of the dataset manifold - there is no constraint our representing the manifold outside the convex hull."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FZxe3HQ9Bd0O",
      "metadata": {
        "id": "FZxe3HQ9Bd0O"
      },
      "outputs": [],
      "source": [
        "# we can print the structure of the model\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vFwDZlQHBd6U",
      "metadata": {
        "id": "vFwDZlQHBd6U"
      },
      "outputs": [],
      "source": [
        "# cache for network activations\n",
        "activations = {}\n",
        "\n",
        "# define a forward pass hook to grab the outputs\n",
        "def make_hook(name):\n",
        "  def hook(module, input, output):\n",
        "    # detach to avoid keeping the full graph\n",
        "    activations[name] = output.detach().cpu()\n",
        "  return hook\n",
        "\n",
        "# register model hooks to the ReLU outputs\n",
        "for idx, layer in enumerate(model.layers):\n",
        "  if not layer.act._forward_hooks:\n",
        "    layer.act.register_forward_hook(make_hook(f\"layer_{idx}_relu\"))\n",
        "\n",
        "# run a forward pass on region to populate activations\n",
        "x = torch.linspace(-1, 2, steps=1000).unsqueeze(-1)\n",
        "_ = model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RYZHknILBtYE",
      "metadata": {
        "id": "RYZHknILBtYE"
      },
      "outputs": [],
      "source": [
        "# which perceptron to inspect\n",
        "layer_index = 1\n",
        "perceptron_indexes = [0, 1, 2, 3, 4]\n",
        "\n",
        "# acts is of shape [N, output_dim]\n",
        "fig, ax = plt.subplots(figsize=(8,3))\n",
        "for perceptron_index in perceptron_indexes:\n",
        "  ax.plot(x, activations[f\"layer_{layer_index}_relu\"][:,perceptron_index], label=f\"layer_{layer_index}_relu:{perceptron_index}\")\n",
        "ax.set_title(f\"Perceptron Activations\")\n",
        "ax.set_xlabel(\"x\")\n",
        "ax.set_ylabel(\"activation\")\n",
        "ax.set_xlim(x.min(), x.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus far, we have focused on the general formulation of our function approximation - explored how factors like model capacity, data distribution, and optimization dynamics influence the solutions we learn - and identified some of the limitations inherent in our approach. With the recap done, we will spend the rest of the workshop exploring how we can apply this framework to solve more complex problems..."
      ],
      "metadata": {
        "id": "F_3RgMDRwwXv"
      },
      "id": "F_3RgMDRwwXv"
    },
    {
      "cell_type": "markdown",
      "id": "jZFHEn2Dj5m6",
      "metadata": {
        "id": "jZFHEn2Dj5m6"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0C9uz_BprXhu",
      "metadata": {
        "id": "0C9uz_BprXhu"
      },
      "source": [
        "<h2>Section 1. Problem Formulation</h2>\n",
        "\n",
        "We will focus on applying this framework to different problem settings - in doing so we'll discuss how the structure and representation of our learning task impacts the solutions we can learn and the nature of the insight we can glean.\n",
        "\n",
        "The process of problem formulation is usually extremely problem specific and quite unclear - often this process is guided by our observations of the literature and a healthy dose of creativity/intuition. As we step through it, we will talk about it in an abstract manner - and ground this using a simple example.\n",
        "\n",
        "We're going to explore this in the following steps:\n",
        "0. Investigate the dataset.\n",
        "1. Define the outputs.\n",
        "2. Define our constraints (we'll skip this)\n",
        "3. Formulate an approach...\n",
        "4. Define the inputs.\n",
        "5. Define the hypothesis space.\n",
        "6. Train, evaluate, analyse, and iterate."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cocWHhueRphQ",
      "metadata": {
        "id": "cocWHhueRphQ"
      },
      "source": [
        "<h3>Section 1.0 Spiral Arm Classification</h3>\n",
        "\n",
        "Let's consider a problem setting - we're working with a team to analyse the structure of the arms of spiral galaxies - our objective is to classify which arm of a spiral dataset each star belongs to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MNhbVlB4RtKF",
      "metadata": {
        "id": "MNhbVlB4RtKF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Spiral():\n",
        "  def __init__(self, N: int = 100, noise: float = 0.2, ratio: float = 1.0, angle: float = .0):\n",
        "    # angle\n",
        "    theta = 2 * np.pi * ratio * np.random.rand(N)\n",
        "    r_a = 2 * theta + np.pi / 2\n",
        "    r_b = -2 * theta - np.pi / 2\n",
        "\n",
        "    # Spiral A (label 0)\n",
        "    x_a = np.stack([np.cos(theta) * r_a, np.sin(theta) * r_a], axis=1)\n",
        "    x_a += np.random.randn(N, 2) * noise\n",
        "    y_a = np.zeros(N)\n",
        "\n",
        "    # Spiral B (label 1)\n",
        "    x_b = np.stack([np.cos(theta) * r_b, np.sin(theta) * r_b], axis=1)\n",
        "    x_b += np.random.randn(N, 2) * noise\n",
        "    y_b = np.ones(N)\n",
        "\n",
        "    # format\n",
        "    x = np.concatenate([x_a, x_b], axis=0)\n",
        "    y = np.concatenate([y_a, y_b], axis=0)\n",
        "    x = torch.tensor(x, dtype=torch.float32)\n",
        "    y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # for BCE loss\n",
        "\n",
        "    # rotate\n",
        "    if angle != 0.0:\n",
        "      c, s = np.cos(angle), np.sin(angle)\n",
        "      R = torch.tensor([[c, -s], [s,  c]], dtype=torch.float32)\n",
        "      x = x @ R.T\n",
        "\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return self.x.shape[0]\n",
        "\n",
        "  def __getitem__(self, index: int) -> tuple:\n",
        "    return self.x, self.y\n",
        "\n",
        "\n",
        "class SpiralDataset():\n",
        "  \"\"\"\n",
        "  Dataset containing multiple randomly generated spirals.\n",
        "  Returns:\n",
        "      x: Tensor of shape [2N, 2]\n",
        "      y: Tensor of shape [2N] with labels 0 or 1\n",
        "  \"\"\"\n",
        "  def __init__(self, n_spirals: int, points: list, noise: list, ratio: list, angle: list) -> None:\n",
        "    #\n",
        "    self.spirals = [Spiral(\n",
        "        N=int(np.random.uniform(points[0], points[1])),\n",
        "        noise=np.random.uniform(noise[0], noise[1]),\n",
        "        ratio=np.random.uniform(ratio[0], ratio[1]),\n",
        "        angle=np.random.uniform(angle[0], angle[1])\n",
        "    ) for _ in range(n_spirals)]\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return len(self.spirals)\n",
        "\n",
        "  def __getitem__(self, index: int) -> tuple:\n",
        "    spiral = self.spirals[index]\n",
        "    return spiral.x, spiral.y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tDsiHo7Nyi_r",
      "metadata": {
        "id": "tDsiHo7Nyi_r"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# define a single spiral\n",
        "dataset = SpiralDataset(n_spirals=1, points=[100,200], noise=[0.2, 0.5], ratio=[1.00, 1.25], angle=[.0, np.pi/2])\n",
        "\n",
        "# visualize the dataset\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "mask0 = dataset.spirals[0].y.squeeze() == 0 # class A\n",
        "ax.scatter(dataset.spirals[0].x[mask0, 0], dataset.spirals[0].x[mask0, 1], color=\"darkblue\", label=\"Class A\", edgecolor=\"black\")\n",
        "mask1 = dataset.spirals[0].y.squeeze() == 1 # class B\n",
        "ax.scatter(dataset.spirals[0].x[mask1, 0], dataset.spirals[0].x[mask1, 1], color=\"darkred\", label=\"Class B\", edgecolor=\"black\")\n",
        "ax.set_title(\"Spiral Dataset\")\n",
        "ax.grid(True, alpha=0.50)\n",
        "ax.set_xlabel(r\"$x_{0}$\")\n",
        "ax.set_ylabel(r\"$x_{1}$\")\n",
        "ax.legend(loc=\"best\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x0yF6I--wWL2",
      "metadata": {
        "id": "x0yF6I--wWL2"
      },
      "source": [
        "We can investigate the structure of the spiral and observe that we have $N$ points, and each points has an associated $x$ and $y$:\n",
        "\n",
        "\\begin{align*}\n",
        "  x \\in \\mathbb{R}^{2}, \\quad y \\in \\{0, 1\\}\n",
        "\\end{align*}\n",
        "\n",
        "where $x = (x_{0}, x_{1})$ represents the position of a sample in cartesian coordinates and $y \\in \\{0, 1\\}$ represents the class label of the position i.e. spiral A (class label 0) or B (class label 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KpW7imqdxq1R",
      "metadata": {
        "id": "KpW7imqdxq1R"
      },
      "outputs": [],
      "source": [
        "# how is the data stored?\n",
        "print(f\"x shape: {dataset.spirals[0].x.shape}, dtype: {dataset.spirals[0].x.dtype}\")\n",
        "print(f\"y shape: {dataset.spirals[0].y.shape}, dtype: {dataset.spirals[0].y.dtype}\")\n",
        "print(\"\")\n",
        "\n",
        "# what does a single sample look like?\n",
        "print(f\"X_0 = (x_0, x_1) = {dataset.spirals[0].x[0]}\")\n",
        "print(f\"Y_0 = {dataset.spirals[0].y[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LlSWpXVRepJM",
      "metadata": {
        "id": "LlSWpXVRepJM"
      },
      "source": [
        "<h3>Section 1.1 Defining the Outputs</h3>\n",
        "\n",
        "Given the goal of classifying which spiral arm each star in the dataset belongs to - the first question to ask is what specific output from the model do we want?\n",
        "\n",
        "*   Predict a single class label associated to each point - which class does the point belong to?\n",
        "*   Predict a probability distribution across classes - how much does a given point belong to a single class?\n",
        "*   Predict a class label/distribution for a given region - how much does a given region belong to a class?\n",
        "*   ...\n",
        "\n",
        "Each approach uses the same data and achieves the same goal but approaches it in a slightly different manner. Choosing the right formulation is a design decision that interacts with your problem setting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_YjlvnZh5Alk",
      "metadata": {
        "id": "_YjlvnZh5Alk"
      },
      "source": [
        "üìå Let's assume we want to directly predict which class a point belongs to - hard classification. We want some function $f$, that takes in some input $x$, that directly predicts the class label $y \\in \\{0, 1\\}$:\n",
        "\n",
        "\\begin{align*}\n",
        "  f: x \\in ... \\mapsto y \\in \\{0, 1\\}\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03-GpPP3-OJs",
      "metadata": {
        "id": "03-GpPP3-OJs"
      },
      "outputs": [],
      "source": [
        "# our dataset is already stored in this format\n",
        "dataset.spirals[0].y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U1uKVK4qLxu2",
      "metadata": {
        "id": "U1uKVK4qLxu2"
      },
      "outputs": [],
      "source": [
        "torch.unique(dataset.spirals[0].y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KidVOlfYP4XT",
      "metadata": {
        "id": "KidVOlfYP4XT"
      },
      "source": [
        "üí¨ *What do you want your model to achieve? What is the specific output of your model and what does it represent? How does that output help achieve your task?*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Section 1.2 Problem Formulation</h3>\n",
        "\n",
        "How do you want to go about achieving this output? There are many valid ways to formulate a learning problem, even with the same data - different formulations represent slight different objectives and interpretations of the problem:\n",
        "\n",
        "You might decide a direct formulation is the best approach:\n",
        "\n",
        "*   Directly predict the specified output.\n",
        "\n",
        "You might have constraints in your problem setting such as limited labelled data, and so a self-supervised pre-training step is the best approach:\n",
        "\n",
        "*   Pre-train a model using masked feature prediction - then fine-tune a probe on the features.\n",
        "*   Learn a compressed representation of the spiral - perform clustering on the latent state.\n",
        "\n",
        "This process of problem formulation is probably the most unclear aspect of machine learning - what is the most suitable inductive bias for your setting - what do you want to achieve? I find this requires a strong understanding of the problem setting coupled, broad awareness of different methods, and a healthy dose of creativity/intuition.\n",
        "\n",
        "üìå We have perfectly labeled data and so we will directly predict the class label for a given point:\n",
        "\n",
        "\\begin{align*}\n",
        "  f: x \\in ... \\mapsto y \\in \\{0, 1\\}\n",
        "\\end{align*}\n",
        "\n",
        "üí¨ *What are the constraints of your problem setting? How does your proposed approach couple with this?*"
      ],
      "metadata": {
        "id": "7vtSURJdVsX2"
      },
      "id": "7vtSURJdVsX2"
    },
    {
      "cell_type": "markdown",
      "id": "7-pAO8L5iZIN",
      "metadata": {
        "id": "7-pAO8L5iZIN"
      },
      "source": [
        "<h3>Section 1.3 How should we structure our inputs?</h3>\n",
        "\n",
        "With the desired output and the process to achieve the output specified we should also consider how to structure the input. This is a critical step in the design of machine learning systems that bakes in strong assumptions about:\n",
        "\n",
        "*   What kind of structure we believe the data has.\n",
        "*   What kind of operations we want the model to learn.\n",
        "*   What biases we want to encode into the hypothesis space."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider we want to predict a probability distribution across classes for regions in input space, we also determine that global context of the spiral is important. Thus, we might decide to rasterize the spiral dataset aka. treating it as an image...\n",
        "\n",
        "However, as a result of making this choice we now have to impose further assumptions. What should the spatial resolution of our input be - how does this generalize across different types of spirals?"
      ],
      "metadata": {
        "id": "hAk2esU3Y6xL"
      },
      "id": "hAk2esU3Y6xL"
    },
    {
      "cell_type": "code",
      "source": [
        "# rasterize the data\n",
        "def rasterize_by_class(X, y, bins=100):\n",
        "    x0_min, x0_max = X[:,0].min(), X[:,0].max()\n",
        "    x1_min, x1_max = X[:,1].min(), X[:,1].max()\n",
        "\n",
        "    # Separate class 0 and class 1\n",
        "    X0 = X[y.squeeze() == 0]\n",
        "    X1 = X[y.squeeze() == 1]\n",
        "\n",
        "    hist0, _, _ = np.histogram2d(X0[:,0], X0[:,1], bins=bins, range=[[x0_min, x0_max], [x1_min, x1_max]])\n",
        "    hist1, _, _ = np.histogram2d(X1[:,0], X1[:,1], bins=bins, range=[[x0_min, x0_max], [x1_min, x1_max]])\n",
        "\n",
        "    # Normalize histograms to [0,1] for color intensity\n",
        "    hist0 /= hist0.max()\n",
        "    hist1 /= hist1.max()\n",
        "\n",
        "    # Combine into RGB image: Red for class 1, Blue for class 0\n",
        "    rgb = np.zeros((bins, bins, 3))\n",
        "    rgb[..., 0] = hist1.T  # Red channel\n",
        "    rgb[..., 2] = hist0.T  # Blue channel\n",
        "\n",
        "    return rgb, (x0_min, x0_max, x1_min, x1_max)\n",
        "\n",
        "# define a single spiral\n",
        "dataset = SpiralDataset(n_spirals=1, points=[100,200], noise=[0.2, 0.5], ratio=[1.00, 1.00], angle=[.0, np.pi/2])\n",
        "\n",
        "# rasterize\n",
        "rgb, extent = rasterize_by_class(dataset.spirals[0].x, dataset.spirals[0].y, bins=20)\n",
        "\n",
        "# plot image\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "ax.imshow(rgb, extent=extent)\n",
        "ax.set_title(\"Rasterized Spiral (Image-like)\")\n",
        "ax.set_xlabel(\"$x_0$\")\n",
        "ax.set_ylabel(\"$x_1$\")\n",
        "ax.grid(True, alpha=0.50)"
      ],
      "metadata": {
        "id": "gVmvEeluZN0R"
      },
      "id": "gVmvEeluZN0R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We might be trying to predict a single class label for each point. We know what spiral a point belongs to depends on the global context and the geometry between its neighbours - so we might decide to represent the spirals as graphs.\n",
        "\n",
        "However, this introduces it's own set of subsequent assumptions (choices) we have to make. How do we decide what points to link together - how does this relate to what we might expect to encounter during inference?"
      ],
      "metadata": {
        "id": "EtzUzr-DZ-j4"
      },
      "id": "EtzUzr-DZ-j4"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import kneighbors_graph\n",
        "import networkx as nx\n",
        "\n",
        "A = kneighbors_graph(dataset.spirals[0].x, n_neighbors=6, mode='connectivity')\n",
        "G = nx.from_scipy_sparse_array(A)\n",
        "\n",
        "pos = {i: (dataset.spirals[0].x[i, 0], dataset.spirals[0].x[i, 1]) for i in range(len(dataset.spirals[0].x))}\n",
        "colors = ['blue' if label == 0 else 'red' for label in dataset.spirals[0].y]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "nx.draw(G, pos, node_color=colors, node_size=20, edge_color='gray', ax=ax)\n",
        "ax.set_title(\"Graph View (k-NN)\")\n",
        "ax.set_xlabel(\"$x_0$\")\n",
        "ax.set_ylabel(\"$x_1$\")\n",
        "ax.grid(True, alpha=0.50)"
      ],
      "metadata": {
        "id": "UZM22IwjavV2"
      },
      "id": "UZM22IwjavV2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7FwkJS0X9wGg",
      "metadata": {
        "id": "7FwkJS0X9wGg"
      },
      "source": [
        "> Each representation encodes different assumptions about the structure of the data bringing with it pros and cons."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y0p2nl0H-Szq",
      "metadata": {
        "id": "y0p2nl0H-Szq"
      },
      "source": [
        "üìå For our spiral dataset, we'll assume our input is a single point representation - that is each input is simply a pair of coordinates in continuous space using Cartesian coordinates:\n",
        "\n",
        "\\begin{align*}\n",
        "  x = (x_{0}, x_{1}) \\in \\mathbb{R}^{2}\n",
        "\\end{align*}\n",
        "\n",
        "This feels like a reasonable representation for our data - it has the benefit of simplicity - but has the downsides of no local/global context. Hence, we have defined our input:\n",
        "\n",
        "\\begin{align*}\n",
        "  f : x \\in \\mathbb{R}^{2} \\mapsto y \\in \\{0, 1\\}\n",
        "\\end{align*}\n",
        "\n",
        "Within this formulation we're asking our model to project points in one spiral arm to $0$ and the other spiral arm to $1$ - separate the two classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NKart0r-RJjo",
      "metadata": {
        "id": "NKart0r-RJjo"
      },
      "outputs": [],
      "source": [
        "# visualize the dataset\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "mask0 = dataset.spirals[0].y.squeeze() == 0\n",
        "ax.scatter(dataset.spirals[0].x[mask0, 0], dataset.spirals[0].x[mask0, 1], color=\"darkblue\", label=\"Class A\", edgecolor=\"black\")\n",
        "mask1 = dataset.spirals[0].y.squeeze() == 1\n",
        "ax.scatter(dataset.spirals[0].x[mask1, 0], dataset.spirals[0].x[mask1, 1], color=\"darkred\", label=\"Class B\", edgecolor=\"black\")\n",
        "ax.set_title(\"Spiral Dataset\")\n",
        "ax.grid(True, alpha=0.50)\n",
        "ax.set_xlabel(r\"$x_{0}$\")\n",
        "ax.set_ylabel(r\"$x_{1}$\")\n",
        "ax.legend(loc=\"best\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Sn2d6xQTQQWZ",
      "metadata": {
        "id": "Sn2d6xQTQQWZ"
      },
      "source": [
        "üí¨ *How do you represent/structure the inputs to your model? What assumptions does that representation encode? What are the pros/cons?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cmjXi-avlSUS",
      "metadata": {
        "id": "cmjXi-avlSUS"
      },
      "source": [
        "You might also extend this discussion on how we represent our data to include pre-processing steps such as normalization - these sort of steps adjust the structure and distribution of the input and interacts strongly with aspects of our modelling approach such as parameter initialization, which in turn influence what and how we learn."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MeujwBHbnMSn",
      "metadata": {
        "id": "MeujwBHbnMSn"
      },
      "source": [
        "<h3>Section 1.4 Hypothesis Space</h3>\n",
        "\n",
        "So far we have defined how the input and output data is represented. Now we can decide what sort of hypothesis space will we chose to learn this function? Each representation will typically require a slightly different hypothesis space to handle the structure of the data - thus different architectures of models are used:\n",
        "\n",
        "*   Images $\\rightarrow$ CNNs, Transformers, etc.\n",
        "*   Point clouds $\\rightarrow$ MLPs, Transformers, etc.\n",
        "*   Graphs $\\rightarrow$ GNNs, MPNNs, Transformers, etc.\n",
        "*   Sequences $\\rightarrow$ RNNs, Transformers, etc.\n",
        "\n",
        "Each architecture also has it's own pros and cons that relate to how their inductive biases are designed - this also interacts with the sorts of solutions you can learn."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AlW2AV8QSXCg",
      "metadata": {
        "id": "AlW2AV8QSXCg"
      },
      "source": [
        "üìå We have been working so far with multi-layer perceptrons, this is a reasonable model class to use in this scenario. We want our MLP, $f$\n",
        "\n",
        "\\begin{align*}\n",
        "  y = f(x) = MLP(x) = Layer_{N}(Layer_{...}(Layer_{0}(x)))\n",
        "\\end{align*}\n",
        "\n",
        "to take as input the 2D coordinates:\n",
        "\n",
        "\\begin{align*}\n",
        "  x = (x_{0}, x_{1}) \\in \\mathbb{R}^{2}\n",
        "\\end{align*}\n",
        "\n",
        "and output a scalar value representing the class label.\n",
        "\n",
        "\\begin{align*}\n",
        "  y \\in \\{0, 1\\}\n",
        "\\end{align*}\n",
        "\n",
        "We can define a network architecture that achieves this by abstracting our previous model architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CX28qArC6cfh",
      "metadata": {
        "id": "CX28qArC6cfh"
      },
      "outputs": [],
      "source": [
        "\"\"\" reference model\n",
        "class MultiLayerPerceptron(nn.Module):\n",
        "  def __init__(self, hidden_layers: int = 0, hidden_dim: int = 1, input_dim: int = 1, output_dim: int = 1):\n",
        "    super(MultiLayerPerceptron, self).__init__()\n",
        "    modules = [PerceptronLayer(input_dim, hidden_dim)] # input layer\n",
        "    for _ in range(hidden_layers): modules.append(PerceptronLayer(hidden_dim, hidden_dim)) # hidden layers\n",
        "    modules.append(PerceptronLayer(hidden_dim, output_dim, act=False)) # output layer (no activation)\n",
        "    self.layers = nn.ModuleList(modules) # store layers\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\"\"\"\n",
        "\n",
        "# design your own model...\n",
        "class PointClassificationMultiLayerPerceptron(nn.Module):\n",
        "  def __init__(self, hidden_layers: int = 0, hidden_dim: int = 1):\n",
        "    # how many inputs...\n",
        "    # how many outputs...\n",
        "    ...\n",
        "\n",
        "  def forward(self, x):\n",
        "    # apply model layers...\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8g73-hUIUF8h",
      "metadata": {
        "id": "8g73-hUIUF8h"
      },
      "source": [
        "<h3>Section 1.5 Training the Model</h3>\n",
        "\n",
        "We can go ahead and train our model on our dataset; this might take some tweaking of the model and optimization process..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FgNnX75rZAO0",
      "metadata": {
        "id": "FgNnX75rZAO0"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, optimizer, loss_fn, dataset, steps):\n",
        "  losses = []\n",
        "  with tqdm(range(steps)) as pbar:\n",
        "    for idx in pbar:\n",
        "      assert len(dataset) == 1\n",
        "      x, y = dataset.__getitem__(0)\n",
        "      optimizer.zero_grad()\n",
        "      y_pred = model(x)\n",
        "      loss = loss_fn(y_pred, y) # input=logits, target=labels\n",
        "      losses.append(loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      pbar.set_description(f\"loss: {loss.item():.3f}\")\n",
        "    return model, torch.tensor(losses)\n",
        "\n",
        "def plot_predictions(model, spiral):\n",
        "    # evaluate model\n",
        "    with torch.no_grad():\n",
        "        logits = model(spiral.x)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = (probs >= 0.5).float().cpu().numpy().squeeze()\n",
        "\n",
        "    # Prepare data\n",
        "    x_np = spiral.x.cpu().numpy()\n",
        "\n",
        "    # Plot predictions (color = model prediction)\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    ax.scatter(x_np[:, 0], x_np[:, 1], c=preds, cmap=\"RdBu\", edgecolor=\"k\", label=\"Predictions\", alpha=0.75)\n",
        "\n",
        "    ax.set_title(\"Model Predictions (Hard Classification)\")\n",
        "    ax.set_xlabel(r\"$x_0$\")\n",
        "    ax.set_ylabel(r\"$x_1$\")\n",
        "    ax.set_aspect(\"equal\")\n",
        "    ax.legend(loc=\"best\")\n",
        "    ax.grid(True, alpha=0.50)\n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GTXdCOR-UXNI",
      "metadata": {
        "id": "GTXdCOR-UXNI"
      },
      "outputs": [],
      "source": [
        "# 1. Define dataset\n",
        "dataset = SpiralDataset(n_spirals=1, points=[100,200], noise=[0.2, 0.5], ratio=[1.00, 1.00], angle=[.0, np.pi/2])\n",
        "\n",
        "# 2. Define model\n",
        "model = PointClassificationMultiLayerPerceptron(hidden_layers=0, hidden_dim=2)\n",
        "\n",
        "# 3. Train the model to fit the dataset\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "model, losses = training_loop(model, optimizer, loss_fn, dataset, steps=100)\n",
        "\n",
        "# 4. Analyse the results\n",
        "_ = plot_loss_curve(losses)\n",
        "_ = plot_predictions(model, dataset.spirals[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P14DDAiyZVp9",
      "metadata": {
        "id": "P14DDAiyZVp9"
      },
      "source": [
        "We can see we might be getting some reasonable predictions now. An interesting question to ask is what does the model predict across the entire domain i.e. outside of the spirals?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YzUF3R-3ZiCO",
      "metadata": {
        "id": "YzUF3R-3ZiCO"
      },
      "outputs": [],
      "source": [
        "def plot_spiral_classification(\n",
        "  model,\n",
        "  spiral,\n",
        "  grid_range=(-25, 25),\n",
        "  grid_size=500,\n",
        "  levels=10,\n",
        "):\n",
        "  \"\"\"\n",
        "  Plot model predictions over the 2D spiral input space alongside the true data points.\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  # Create meshgrid over specified range\n",
        "  x_min, x_max = grid_range\n",
        "  y_min, y_max = grid_range\n",
        "  xx, yy = torch.meshgrid(\n",
        "      torch.linspace(x_min, x_max, grid_size),\n",
        "      torch.linspace(y_min, y_max, grid_size),\n",
        "      indexing='ij'\n",
        "  )\n",
        "  grid = torch.stack([xx.flatten(), yy.flatten()], dim=1)\n",
        "\n",
        "  # Predict probabilities on grid\n",
        "  with torch.no_grad():\n",
        "      logits = model(grid)\n",
        "      probs = torch.sigmoid(logits).reshape(xx.shape).cpu().numpy()\n",
        "\n",
        "  # Plot contour of predicted probabilities\n",
        "  fig, ax = plt.subplots(figsize=(6, 6))\n",
        "  contour = ax.contourf(\n",
        "      xx.cpu().numpy(), yy.cpu().numpy(), probs,\n",
        "      levels=levels, cmap=\"RdBu\", alpha=0.50\n",
        "  )\n",
        "\n",
        "  # Overlay true data points\n",
        "  X_data = spiral.x.cpu().numpy()\n",
        "  y_data = spiral.y.squeeze().cpu().numpy()\n",
        "  ax.scatter(\n",
        "      X_data[:, 0], X_data[:, 1],\n",
        "      c=y_data, cmap=\"RdBu\", edgecolor='k'\n",
        "  )\n",
        "\n",
        "  ax.set_title(\"Spiral Classification\")\n",
        "  ax.set_xlim(x_min, x_max)\n",
        "  ax.set_ylim(y_min, y_max)\n",
        "  ax.set_xlabel(r\"$x_0$\")\n",
        "  ax.set_ylabel(r\"$x_1$\")\n",
        "  ax.set_aspect(\"equal\")\n",
        "  plt.tight_layout()\n",
        "  return fig, ax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xNl0TaT9aPzt",
      "metadata": {
        "id": "xNl0TaT9aPzt"
      },
      "outputs": [],
      "source": [
        "_ = plot_spiral_classification(model, dataset.spirals[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WrcJ_mFqgLY2",
      "metadata": {
        "id": "WrcJ_mFqgLY2"
      },
      "source": [
        "We can think about what we're asking the network to do here, and how this shapes what it learns. We can think of the network as learning a height function (a surface) over the 2D input plane:\n",
        "\n",
        "\\begin{align*}\n",
        "  y = f(x_{0}, x_{1})\n",
        "\\end{align*}\n",
        "\n",
        "In its raw form, the spirals are not linearly separable - theres no straight line we can draw to separate the classes. But, once we ‚Äúlift‚Äù each point up to its height $y$, the spirals trace out two separate ridges on the surface. A simple horizontal cut‚Äîsay at $y = 0.5$ now cleanly slices the surface into two regions, each containing exactly one spiral. This enables classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ssel3-agUcv",
      "metadata": {
        "id": "2ssel3-agUcv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "\n",
        "def plot_spiral_classification_3d(\n",
        "    model,\n",
        "    spiral,\n",
        "    grid_range=(-25, 25),\n",
        "    grid_size=500,\n",
        "    cmap=\"RdBu\",\n",
        "    alpha=0.8,\n",
        "    point_size=20\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot model predictions as a 3D surface over the 2D spiral input space,\n",
        "    with true data points projected onto the surface.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 1) Create meshgrid\n",
        "    x_min, x_max = grid_range\n",
        "    y_min, y_max = grid_range\n",
        "    xx = np.linspace(x_min, x_max, grid_size)\n",
        "    yy = np.linspace(y_min, y_max, grid_size)\n",
        "    XX, YY = np.meshgrid(xx, yy)\n",
        "    grid = np.stack([XX.ravel(), YY.ravel()], axis=1)\n",
        "    grid_t = torch.tensor(grid, dtype=torch.float32)\n",
        "\n",
        "    # 2) Predict\n",
        "    with torch.no_grad():\n",
        "        logits = model(grid_t)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy().reshape(XX.shape)\n",
        "\n",
        "    # 3) Plot\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "    # Surface\n",
        "    surf = ax.plot_surface(\n",
        "        XX, YY, probs,\n",
        "        rstride=5, cstride=5,\n",
        "        cmap=cmap, edgecolor='none', alpha=alpha\n",
        "    )\n",
        "\n",
        "    # Scatter data points at their true probabilities (0 or 1)\n",
        "    Xd = spiral.x.cpu().numpy()\n",
        "    yd = spiral.y.squeeze().cpu().numpy()\n",
        "    # Evaluate model at data points for actual height\n",
        "    with torch.no_grad():\n",
        "        dp = torch.sigmoid(model(spiral.x)).cpu().numpy().squeeze()\n",
        "    ax.scatter(\n",
        "        Xd[:, 0], Xd[:, 1], dp,\n",
        "        c=yd, cmap=\"RdBu\",\n",
        "        edgecolor='k', s=point_size,\n",
        "        label=\"Data Predictions\"\n",
        "    )\n",
        "\n",
        "    ax.set_title(\"3D Spiral Classification Surface\")\n",
        "    ax.set_xlabel(\"$x_0$\")\n",
        "    ax.set_ylabel(\"$x_1$\")\n",
        "    ax.set_zlabel(\"Predicted Probability\")\n",
        "    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label=\"P(class=1)\")\n",
        "    ax.view_init(elev=30, azim=-60)\n",
        "    plt.tight_layout()\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "fig, ax = plot_spiral_classification_3d(model, dataset.spirals[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ds1qXrN7hFBF",
      "metadata": {
        "id": "ds1qXrN7hFBF"
      },
      "source": [
        "In other words, rather than trying to draw a winding curve in the plane, the network learns to warp the data into the third dimension so that a single flat plane can separate the classes. This makes sense - the objective of classification is to separate the classes.\n",
        "\n",
        "This is exactly analogous to how, in 1D, we use ReLU ‚Äúkinks‚Äù to approximate a curve by piecewise-linear segments: in 2D we use ReLU ‚Äúcreases‚Äù to build a surface whose level sets (planes $z = const.$) implement our decision boundary.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z0_3vlB8hVth",
      "metadata": {
        "id": "Z0_3vlB8hVth"
      },
      "source": [
        "In previous workshops we built up an understanding that each Perceptron learns a linear combination of its inputs and applies some non-linear activation function. We can investigate what each neuron is doing in this model too..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qPbqNfJr_C2p",
      "metadata": {
        "id": "qPbqNfJr_C2p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_layer_neuron_response(\n",
        "    model,\n",
        "    spiral,\n",
        "    layer_idx: int = 0,\n",
        "    neuron_idx: int = 0,\n",
        "    bounds: float = 25,\n",
        "    resolution: int = 500,\n",
        "    cmap: str = \"RdBu\",\n",
        "    alpha: float = 0.6\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize the activation of a single neuron in a given PerceptronLayer\n",
        "    over the 2D input plane.\n",
        "\n",
        "    Args:\n",
        "        model:          a MultiLayerPerceptron with .layers (ModuleList of PerceptronLayer)\n",
        "        spiral:        object with .x (Tensor[N,2]) and .y (Tensor[N,1])\n",
        "        layer_idx:      index into model.layers to probe\n",
        "        neuron_idx:     which output neuron of that layer to visualize\n",
        "        bounds:         axes range [-bounds, +bounds]\n",
        "        resolution:     number of grid points per axis\n",
        "        cmap:           heatmap colormap\n",
        "        alpha:          heatmap transparency\n",
        "    Returns:\n",
        "        fig, ax:        Matplotlib figure and axes\n",
        "    \"\"\"\n",
        "    assert hasattr(model, \"layers\"), \"Model must have a .layers attribute\"\n",
        "    assert 0 <= layer_idx < len(model.layers), f\"layer_idx out of range [0, {len(model.layers)-1}]\"\n",
        "\n",
        "    model.eval()\n",
        "    # 1) build grid\n",
        "    xs = torch.linspace(-bounds, bounds, resolution)\n",
        "    ys = torch.linspace(-bounds, bounds, resolution)\n",
        "    xx, yy = torch.meshgrid(xs, ys, indexing=\"ij\")\n",
        "    grid = torch.stack([xx.flatten(), yy.flatten()], dim=1)  # [res^2, 2]\n",
        "\n",
        "    # 2) forward through layers up to layer_idx\n",
        "    with torch.no_grad():\n",
        "        h = grid\n",
        "        for i, layer in enumerate(model.layers):\n",
        "            h = layer(h)\n",
        "            if i == layer_idx:\n",
        "                break\n",
        "        activations = h[:, neuron_idx].reshape(xx.shape).cpu().numpy()\n",
        "\n",
        "    # 3) plot\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    pcm = ax.contourf(\n",
        "        xx.cpu().numpy(),\n",
        "        yy.cpu().numpy(),\n",
        "        activations,\n",
        "        levels=50,\n",
        "        cmap=cmap,\n",
        "        alpha=alpha\n",
        "    )\n",
        "    fig.colorbar(pcm, ax=ax, label=f\"Layer {layer_idx} Neuron {neuron_idx} Activation\")\n",
        "\n",
        "    # overlay data\n",
        "    X_data = spiral.x.cpu().numpy()\n",
        "    y_data = spiral.y.squeeze().cpu().numpy()\n",
        "    ax.scatter(\n",
        "        X_data[:, 0], X_data[:, 1],\n",
        "        c=y_data, cmap=cmap,\n",
        "        edgecolor=\"k\", s=15, label=\"Data\"\n",
        "    )\n",
        "\n",
        "    ax.set_title(f\"Activation of Neuron {neuron_idx} in Layer {layer_idx}\")\n",
        "    ax.set_xlim(-bounds, bounds)\n",
        "    ax.set_ylim(-bounds, bounds)\n",
        "    ax.set_xlabel(\"$x_0$\")\n",
        "    ax.set_ylabel(\"$x_1$\")\n",
        "    ax.set_aspect(\"equal\")\n",
        "    ax.legend(loc=\"best\")\n",
        "    plt.tight_layout()\n",
        "    return fig, ax\n",
        "\n",
        "fig, ax = plot_layer_neuron_response(\n",
        "    model=model,\n",
        "    spiral=dataset.spirals[0],\n",
        "    layer_idx=0,\n",
        "    neuron_idx=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UPfG1byjj17l",
      "metadata": {
        "id": "UPfG1byjj17l"
      },
      "source": [
        "In a $D$ dimensional input space, an MLP with ReLU activations alternates linear projections (affine mappings h=Wx+b) with folds (ReLU clamps along each coordinate‚Äôs hyperplane). This composition partitions $\\mathbb{R}^{D}$ into a mosaic of convex polytopes, within each of which the network's overall mapping is exactly affine. By stacking many project-and-fold layers, deep nets ‚Äúuntangle‚Äù complex manifolds so that a single final hyperplane can cleanly separate the classes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qw8hZu1Qk82S",
      "metadata": {
        "id": "qw8hZu1Qk82S"
      },
      "source": [
        "*An aside on manifolds...*\n",
        "\n",
        "Real-world data‚Äîwhether images, audio, or text‚Äîrarely fills its high-dimensional input space uniformly. Instead, it lies on a much lower-dimensional manifold, a ‚Äúsurface‚Äù defined by the data's true degrees of freedom.\n",
        "\n",
        "Learning this manifold means discovering that hidden subspace so that the model focuses on the essential structure (e.g. semantic content or object shape) rather than irrelevant variation.\n",
        "\n",
        "Deep networks uncover and flatten these manifolds by alternating linear projections (which lift data into new feature spaces) and nonlinear folds (ReLU or similar activations that crease along hyperplanes). Once the manifold is well represented, downstream tasks‚Äîclassification, regression, or generation‚Äîboil down to simple geometric operations (like slicing with a hyperplane or interpolating along the surface) in that learned feature space."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Mu2NhqPIkNut",
      "metadata": {
        "id": "Mu2NhqPIkNut"
      },
      "source": [
        "<h3>Section 1.6 Analyse the results</h3>\n",
        "\n",
        "We can explore how well our model generalizes to different scenarios from the context of interpolation and extrapolation.\n",
        "\n",
        "<!-- <h4>A. How does class noise impact performance?</h4>\n",
        "\n",
        "When class manifolds occupy distinct, well-separated regions‚Äîlike two spirals winding far apart‚Äîa simple boundary suffices and even a simple MLP can separate them.\n",
        "\n",
        "But when classes lie in close proximity or overlap on the manifold - imagine trying to classify different types of cars versus dogs and cars - the model must learn much more complex, nonlinear decision surfaces to untangle and distinguish them.\n",
        "\n",
        "We can imagine we have noise in our dataset which means the decision boundary is much tighter. Or perhaps we don't have classes that we can truly separate. -->\n",
        "\n",
        "<!-- # 1. Define dataset\n",
        "dataset = SpiralDataset(n_spirals=1, points=[100,200], noise=[0.2, 0.5], ratio=[1.00, 1.00], angle=[.0, np.pi/2])\n",
        "\n",
        "# 2. Define the model\n",
        "model = PointClassificationMultiLayerPerceptron(hidden_layers=0, hidden_dim=10)\n",
        "\n",
        "# 3. Train the model to fit the dataset\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "model, losses = training_loop(model, optimizer, loss_fn, dataset, steps=500)\n",
        "\n",
        "# 4. Analyse the results\n",
        "_ = plot_spiral_classification(model, dataset.spirals[0]) -->\n",
        "\n",
        "<!-- As before we can observe how the network learns to interpolate in regions where the model has been less constrained. When you remove a region from the training set, the MLP has no information about how the classes behave there...\n",
        "\n",
        "üí¨ *Given the model learned is a function of the model x dataset x optimization, what techniques could you use to address this issue?* -->\n",
        "\n",
        "<h4>A. How well does the model interpolate?</h4>\n",
        "\n",
        "We can remove a portion of the training data to investigate how sensitive the model performance is to poor coverage of the underlying data manifold. We expect that without data to constrain the function behaviour within the removed region, the ability for the model to interpolate will depend on the geometry of the data manifold - for large changes we expect it to approximate poorly - for small changes we expect it to approximate reasonably."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yJiGKV0xnWm5",
      "metadata": {
        "id": "yJiGKV0xnWm5"
      },
      "outputs": [],
      "source": [
        "# build dataset\n",
        "dataset_full = Spiral(N=100, noise=0.2, ratio=1.0, angle=.0)\n",
        "x_full, y_full = dataset_full.x, dataset_full.y.squeeze()\n",
        "theta = torch.atan2(x_full[:,1], x_full[:,0])\n",
        "\n",
        "# pick a center and width\n",
        "theta_c = 0.75  # radians\n",
        "delta   = np.pi/6  # 22.5¬∞ on each side ‚Üí 45¬∞ total wedge\n",
        "\n",
        "# define masks\n",
        "wedge_mask = (theta > theta_c - delta) & (theta < theta_c + delta)\n",
        "class0 = (y_full == 0)\n",
        "class1 = (y_full == 1)\n",
        "\n",
        "# for *both* classes:\n",
        "train_mask  = (class1 & ~wedge_mask) | class0\n",
        "interp_mask = class1 & wedge_mask\n",
        "\n",
        "# split the dataset up\n",
        "class SubsetDataset:\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "      return 1\n",
        "\n",
        "    def __getitem__(self, *args, **kwargs):\n",
        "      return self.x, self.y\n",
        "\n",
        "train_ds  = SubsetDataset(dataset_full.x[train_mask],  dataset_full.y[train_mask])\n",
        "interp_ds = SubsetDataset(dataset_full.x[interp_mask], dataset_full.y[interp_mask])\n",
        "\n",
        "# train a model\n",
        "model     = PointClassificationMultiLayerPerceptron(hidden_layers=1, hidden_dim=10)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn   = nn.BCEWithLogitsLoss()\n",
        "model, losses = training_loop(model, optimizer, loss_fn, train_ds, steps=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AYC3fc8am948",
      "metadata": {
        "id": "AYC3fc8am948"
      },
      "outputs": [],
      "source": [
        "_ = plot_loss_curve(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H7tr5OXw_8oH",
      "metadata": {
        "id": "H7tr5OXw_8oH"
      },
      "outputs": [],
      "source": [
        "fig, ax = plot_spiral_classification(\n",
        "    model,\n",
        "    train_ds,\n",
        "    grid_range=(-25,25),\n",
        "    grid_size=500,\n",
        "    levels=10\n",
        ")\n",
        "X_int = interp_ds.x.cpu().numpy()\n",
        "y_int = interp_ds.y.squeeze().cpu().numpy()\n",
        "ax.scatter(\n",
        "    X_int[:, 0], X_int[:, 1],\n",
        "    c=y_int, cmap=\"RdBu\",\n",
        "    marker=\"o\", edgecolor=\"yellow\",\n",
        "    s=50, label=\"Held-out wedge\", alpha=0.25\n",
        ")\n",
        "ax.set_title(\"Decision Boundary + Held-out Angular Wedge\")\n",
        "ax.legend(loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üí¨ *Why does this occur?*"
      ],
      "metadata": {
        "id": "xKTCw5GNwOV9"
      },
      "id": "xKTCw5GNwOV9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>C. How well does the model respond to noise?</h3>"
      ],
      "metadata": {
        "id": "eAKmYMTFv27v"
      },
      "id": "eAKmYMTFv27v"
    },
    {
      "cell_type": "code",
      "source": [
        "# build dataset\n",
        "spiral = Spiral(N=100, noise=1.0, ratio=1.0, angle=.0)\n",
        "\n",
        "# plot\n",
        "fig, ax = plot_spiral_classification(\n",
        "    model,\n",
        "    spiral,\n",
        "    grid_range=(-25,25),\n",
        "    grid_size=500,\n",
        "    levels=10\n",
        ")"
      ],
      "metadata": {
        "id": "Zztsc9Ydv6TH"
      },
      "id": "Zztsc9Ydv6TH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üí¨ *Why does this occur?*"
      ],
      "metadata": {
        "id": "9r_xGDuqwQWl"
      },
      "id": "9r_xGDuqwQWl"
    },
    {
      "cell_type": "markdown",
      "id": "a0iMsyRUITb7",
      "metadata": {
        "id": "a0iMsyRUITb7"
      },
      "source": [
        "<h4>C. How well does the model extrapolate?</h4>\n",
        "\n",
        "Because ReLU networks partition space into a finite set of polytopes defined by training-time hyperplanes, any point outside the convex hull of your data simply lands in one of those outermost pieces. The network then applies the same learned affine parameters on that piece, which can lead to constant or unbounded growth (depending on the weights) and typically very poor accuracy.\n",
        "\n",
        "In short, without explicit inductive biases or data covering the extrapolation region, deep nets have no guarantees and tend to perform arbitrarily poorly when asked to extrapolate.\n",
        "\n",
        "To investigate this we might consider extending the spirals..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-YLmJwTpIZDc",
      "metadata": {
        "id": "-YLmJwTpIZDc"
      },
      "outputs": [],
      "source": [
        "# build dataset\n",
        "dataset_extrap = Spiral(N=200, noise=1.0, ratio=1.0, angle=.0)\n",
        "\n",
        "# plot\n",
        "fig, ax = plot_spiral_classification(\n",
        "    model,\n",
        "    dataset_extrap,\n",
        "    grid_range=(-25,25),\n",
        "    grid_size=500,\n",
        "    levels=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BTXZY9i8Pa_p",
      "metadata": {
        "id": "BTXZY9i8Pa_p"
      },
      "source": [
        "üí¨ *How does the model respond to data outside of the convex hull of its training distribution? Why does this occur?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v_3AsSHGIfia",
      "metadata": {
        "id": "v_3AsSHGIfia"
      },
      "source": [
        "<h4>D. How well does the model extrapolate? Again.</h4>\n",
        "\n",
        "When you deploy a model on data whose distribution differs from what it saw during training ‚Äî whether that's a different input range, noise level, or entirely new patterns ‚Äî you have zero theoretical guarantees that its piecewise-affine approximation will hold.\n",
        "\n",
        "In practice, the network will simply apply its learned affine maps to whichever region of its input tessellation the new points fall into, often resulting in unpredictable or completely wrong outputs under distribution shift.\n",
        "\n",
        "To investigate this we might consider flipping the spirals around $x_{0}=0$..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ynwoJ2tP3FVh",
      "metadata": {
        "id": "ynwoJ2tP3FVh"
      },
      "outputs": [],
      "source": [
        "# build dataset\n",
        "dataset_ood = Spiral(N=200, noise=1.0, ratio=1.0, angle=.0)\n",
        "\n",
        "# flip spiral\n",
        "dataset_ood.x[:,0] *= -1 # flip x-coord\n",
        "\n",
        "# plot\n",
        "fig, ax = plot_spiral_classification(\n",
        "    model,\n",
        "    dataset_ood,\n",
        "    grid_range=(-25,25),\n",
        "    grid_size=500,\n",
        "    levels=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PCzDfPx2Pthg",
      "metadata": {
        "id": "PCzDfPx2Pthg"
      },
      "source": [
        "üí¨ *Why does this occur?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TM6OserbD81y",
      "metadata": {
        "id": "TM6OserbD81y"
      },
      "source": [
        "One approach to reduce this issue might be data augmentation. Data augmentation refers to the practice of artificially expanding the dataset (by applying label-preserving transformations) to existing examples. The aim would be to expose the model to a wider data distribution so we hope we are more likely to be operating in the interpolation regime during inference.\n",
        "\n",
        "We might decide to train our model on randomly flipped spiral points..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ucgn5hBKkMVO",
      "metadata": {
        "id": "Ucgn5hBKkMVO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Instantiate your dataset\n",
        "dataset = Spiral(N=200, noise=1.0, ratio=1.0, angle=.0)\n",
        "\n",
        "# 2) Extract numpy arrays\n",
        "X = dataset.x.numpy()          # shape [400, 2]\n",
        "y = dataset.y.numpy().squeeze() # shape [400]\n",
        "\n",
        "# 3) Randomly choose half the points to flip\n",
        "flip_mask = np.random.rand(len(X)) < 0.5\n",
        "X_flipped = X.copy()\n",
        "X_flipped[flip_mask, 0] *= -1\n",
        "\n",
        "# 4) Plot original vs. randomly flipped\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
        "\n",
        "ax1.scatter(X[:, 0], X[:, 1], c=y, cmap=\"coolwarm\", edgecolor=\"k\", s=20)\n",
        "ax1.set_title(\"Original Spiral\")\n",
        "ax1.set_xlabel(\"$x_0$\")\n",
        "ax1.set_ylabel(\"$x_1$\")\n",
        "ax1.set_aspect(\"equal\")\n",
        "\n",
        "ax2.scatter(X_flipped[:, 0], X_flipped[:, 1], c=y, cmap=\"coolwarm\", edgecolor=\"k\", s=20)\n",
        "ax2.set_title(\"Randomly Flipped Points (50%)\")\n",
        "ax2.set_xlabel(\"$x_0$\")\n",
        "ax2.set_aspect(\"equal\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EPlGZvI1lQGL",
      "metadata": {
        "id": "EPlGZvI1lQGL"
      },
      "source": [
        "Data augmentation only helps when the transformations you apply preserve the true label for each input. In our spiral example, an MLP sees only individual points $(x_{0}, x_{1})$ and never ‚Äúknows‚Äù how they connect into spirals. If you rotate or warp points arbitrarily, you'll send some points from spiral A into regions that really belong to spiral B‚Äîso the model gets conflicting examples (same transformed coordinates, different labels), which actually hurts learning rather than helps.\n",
        "\n",
        "To make augmentation effective here, you'd need label-preserving transforms that respect the spiral's geometry‚Äîlike small random jitters along the curve or slight radial scalings that keep points on the same arm. Alternatively, you must give the model global context (e.g. by encoding relative positions along the curve or using sequence-based encoders) so it can distinguish augmented points by where they fall on the overall spiral, not just by their local coordinates.\n",
        "\n",
        "> Effective problem formulation in machine learning means that your choice of inputs, outputs, model architecture, and data processing steps are all deeply interdependent, and each decision you make shapes what and how the model learns."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ctNA2uP_l1zV",
      "metadata": {
        "id": "ctNA2uP_l1zV"
      },
      "source": [
        "üí¨ *How do you use data augmentation in your own research? What specific biases are you hoping to incorporate into your model by doing so?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qRBzBYkSC1AV",
      "metadata": {
        "id": "qRBzBYkSC1AV"
      },
      "source": [
        "<!-- <h3>Section 1G. Mini-batch Gradient Descent</h3>\n",
        "\n",
        "Mini-batch gradient descent is an optimization technique that updates model parameters using the average gradient computed over a small, randomly sampled subset of the training data (the \"mini-batch\") at each step.\n",
        "\n",
        "By processing, say, 32 - 256 examples at once, it leverages parallel hardware for efficiency, smooths out the high variance of per-example updates found in pure stochastic gradient descent, and still retains enough randomness to help escape poor local minima‚Äîunlike full batch methods, which compute gradients over the entire dataset and can be prohibitively slow or memory-intensive on large datasets.\n",
        "\n",
        "When we perform the gradient descent step:\n",
        "\n",
        "\\begin{align*}\n",
        "\\theta := \\theta - \\eta \\cdot \\nabla_\\theta \\mathcal{L}(f_\\theta(x), y)\n",
        "\\end{align*}\n",
        "\n",
        "We compute the loss across a subset of the dataset per step:\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathcal{L}(\\theta)\n",
        "= \\frac{1}{M} \\sum_{i=1}^{M} \\left( f_\\theta(x_i) - y_i \\right)^2\n",
        "\\end{align*}\n",
        "\n",
        "where:\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathcal{D} = \\left\\{ (\\mathcal{X}_{i}, \\mathcal{Y}_{i}) \\right\\}_{i=0}^{M} \\subseteq \\left\\{ (\\mathcal{X}_{i}, \\mathcal{Y}_{i}) \\right\\}_{i=0}^{N}\n",
        "\\end{align*}\n",
        "\n",
        "As your batch size decreases your approximation of the loss landscape becomes worse, and thus your gradient becomes worse. This acts as a regularizing effect during training potentially reducing over-fitting, on the extreme end it can make training extremely difficult.\n",
        "\n",
        "> You're blindfolded and attempting to navigate downhill and the landscape changes each step you take. -->\n",
        "<!--\n",
        "# import matplotlib.pyplot as plt\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# # 1) Instantiate your dataset\n",
        "# dataset = SpiralDataset(N=500, noise=0.20)\n",
        "\n",
        "# # 2) Prepare a dataloader (e.g. batch size 100)\n",
        "# batch_size = 100\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# # 3) Grab one batch\n",
        "# batch_x, batch_y = next(iter(dataloader))\n",
        "\n",
        "# # 4) Convert to NumPy for plotting\n",
        "# full_x = dataset.x.cpu().numpy()\n",
        "# full_y = dataset.y.squeeze().cpu().numpy()\n",
        "# bx     = batch_x.cpu().numpy()\n",
        "# by     = batch_y.squeeze().cpu().numpy()\n",
        "\n",
        "# # 5) Plot\n",
        "# fig, (ax_full, ax_batch) = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
        "\n",
        "# # Full dataset\n",
        "# ax_full.scatter(full_x[:, 0], full_x[:, 1], c=full_y,\n",
        "#                 cmap=\"coolwarm\", edgecolor=\"k\", s=20)\n",
        "# ax_full.set_title(\"Full Spiral Dataset\")\n",
        "# ax_full.set_xlabel(\"$x_0$\")\n",
        "# ax_full.set_ylabel(\"$x_1$\")\n",
        "# ax_full.set_aspect(\"equal\")\n",
        "\n",
        "# # Single batch\n",
        "# ax_batch.scatter(bx[:, 0], bx[:, 1], c=by,\n",
        "#                  cmap=\"coolwarm\", edgecolor=\"k\", s=20)\n",
        "# ax_batch.set_title(f\"Single Batch (size={batch_size})\")\n",
        "# ax_batch.set_xlabel(\"$x_0$\")\n",
        "# ax_batch.set_aspect(\"equal\")\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show() -->\n",
        "\n",
        "\n",
        "<!-- We need to modify our training loop slightly to handle using batches, we will use the `DataLoader` class to handle this for us... -->\n",
        "\n",
        "<!-- # define a training loop\n",
        "def training_loop_epochs(model, optimizer, loss_fn, dataloader, epochs):\n",
        "  losses = []\n",
        "  with tqdm(range(epochs)) as pbar:\n",
        "    for idx in pbar:\n",
        "      for jdx, batch in enumerate(dataloader): # each step represents a subset of the dataset\n",
        "        optimizer.zero_grad()\n",
        "        x, y = batch\n",
        "        y_pred = model(x)\n",
        "        # print(jdx, x.shape, y.shape)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        losses.append(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(f\"[Epoch:{idx+1}/{epochs}] loss @ step {jdx}: {loss.item():.3f}\")\n",
        "  return model, torch.tensor(losses) -->\n",
        "\n",
        "\n",
        "\n",
        "  <!-- We compute the number of epochs by dividing our desired total update count `n_steps` by the number of batches per epoch `N//bs`, so that we perform approximately the same number of update steps as during previous training. Explore how the batch size impacts the performance of the model. -->\n",
        "\n",
        "<!--\n",
        "  from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = SpiralDataset(N=200, noise=0.20)\n",
        "\n",
        "# compute the equivalent number of epochs to use\n",
        "n_steps = 500\n",
        "n_samples = len(dataset) # should be 2*N\n",
        "bs = 32\n",
        "n_epochs = n_steps // (n_samples // bs)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=bs, shuffle=False, drop_last=True)\n",
        "model = PointClassificationMultiLayerPerceptron(hidden_layers=1, hidden_dim=10)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "model, losses = training_loop_epochs(model, optimizer, loss_fn, dataloader, epochs=n_epochs) -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nsziYpdUbx-P",
      "metadata": {
        "id": "nsziYpdUbx-P"
      },
      "source": [
        "<h2>Conclusion</h2>\n",
        "\n",
        "This workshop emphasized how machine learning is more than just model training ‚Äî it is a process of designing learning systems. Every decision and component we use and define encodes assumptions and inductive biases into these systems. The formulation of the problem, the representation of the data, the structure of the model, and the optimization strategy all interact to determine what the model learns.\n",
        "\n",
        "\n",
        "<h3>‚úÖ Takeaways</h3>\n",
        "\n",
        "**From Function Approximation to Classification**: Revisited key ML concepts (hypothesis space, data, optimization), explored universal function approximation with MLPs, and evaluated interpolation vs. extrapolation behaviour.\n",
        "\n",
        "**Problem Formulation & Representation**: Investigated how the structure of a learning problem (e.g., spiral classification) and the representation of inputs (point clouds, images, graphs) affect the choice of model and solution space.\n",
        "\n",
        "**Designing & Evaluating Learning Systems**: Built and trained classification models, visualized their decision surfaces, studied robustness to noise and missing data, and examined limits of generalization through controlled experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wmBMD8_smx4K",
      "metadata": {
        "id": "wmBMD8_smx4K"
      },
      "source": [
        "> We hope you have gained an appreciation both the core mechanisms of deep learning and the trade-offs in designing learning systems."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}